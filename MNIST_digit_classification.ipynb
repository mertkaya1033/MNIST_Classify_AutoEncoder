{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_digit_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO6lVua/4Uhm155t7KacrnM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertkaya1033/MNIST_digit_classify/blob/master/MNIST_digit_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Handwritten Digit Classification\n"
      ],
      "metadata": {
        "id": "ozGR_gDWdZSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Helpers"
      ],
      "metadata": {
        "id": "0zr80rP9kJde"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qZCD4-n8wz9D"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "# https://stackoverflow.com/questions/39832721/meaning-of-self-dict-self-in-a-class-definition\n",
        "class Config(dict):\n",
        "  \"\"\"A class that allows attribute access and item access.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.__dict__ = self\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Digit Classification Trainer"
      ],
      "metadata": {
        "id": "9D283ewe17lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_Classify():\n",
        "  \"\"\"Downloads the MNIST dataset, and trains models that predict the handwritten\n",
        "  digit written on the provided 28x28 image.\"\"\"\n",
        "\n",
        "  _training_data = None\n",
        "  _test_data = None\n",
        "\n",
        "  @staticmethod\n",
        "  def load_data(root: str = \"data\") -> Tuple[Dataset, Dataset]:\n",
        "    \"\"\"Downloads MNIST dataset into local file system, into the root directory. \n",
        "    Loads the dataset into memory.\n",
        "    \n",
        "    Parameters:\n",
        "      root (str) -- the directory to download the dataset. (default: \"data\")\n",
        "    \n",
        "    Returns:\n",
        "      tuple --  (training dataset, test dataset)\n",
        "    \"\"\"\n",
        "    if MNIST_Classify._training_data is None:\n",
        "      MNIST_Classify._training_data = datasets.MNIST(\n",
        "          root=root,\n",
        "          train=True,\n",
        "          download=True,\n",
        "          transform=ToTensor()\n",
        "      )\n",
        "    if MNIST_Classify._test_data is None:\n",
        "      MNIST_Classify._test_data = datasets.MNIST(\n",
        "          root=root,\n",
        "          train=False,\n",
        "          download=True,\n",
        "          transform=ToTensor()\n",
        "      )\n",
        "    return MNIST_Classify._training_data, MNIST_Classify._test_data\n",
        "    \n",
        "  \n",
        "  def _prepare_data(self, args: Config) -> None:\n",
        "    \"\"\"Prepares the dataset for training.\n",
        "\n",
        "    Parameters:\n",
        "        args (Config) --  hyperparameters of the model being trained\n",
        "    \"\"\"\n",
        "    print(\"Preparing Data...\")\n",
        "    MNIST_Classify.load_data()\n",
        "\n",
        "    self._train_dataloader = DataLoader(MNIST_Classify._training_data, batch_size=args.batch_size, shuffle=True)\n",
        "    self._test_dataloader = DataLoader(MNIST_Classify._test_data, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "  def train_model(self, args: Config):\n",
        "    \"\"\"Trains an instance of a model provided in args with the given hyperparameters\n",
        "    for the task of classifying handwritten digits using the MNIST dataset.\n",
        "\n",
        "    Parameters:\n",
        "        args (Config) --  stores the model and the hyperparameters in which the \n",
        "                          model instance should be trained\n",
        "\n",
        "    Returns:\n",
        "        The trained model instance\n",
        "    \"\"\"\n",
        "    print(\"===================\", args.experiment_name, \"===================\")\n",
        "\n",
        "    \n",
        "    torch.manual_seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    \n",
        "    self._prepare_data(args)\n",
        "    \n",
        "    num_digits = 10\n",
        "    num_channels = 1\n",
        "    image_size = MNIST_Classify._training_data.data.size()[1] #28x28\n",
        "    num_batches = len(self._train_dataloader)\n",
        "\n",
        "    mnist_model = args.Model(image_size, num_digits, num_channels, args)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(params=mnist_model.parameters(), lr=args.learn_rate)\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    print(\"Beginning Training...\")\n",
        "    start_time = time.time()\n",
        "    for epoch in range(args.epochs):\n",
        "      \n",
        "      mnist_model.train()\n",
        "      losses = []\n",
        "      \n",
        "      # Training\n",
        "      for i, (input, label) in enumerate(self._train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = mnist_model(input)\n",
        "        loss = loss_func(predictions, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.data.item())\n",
        "      \n",
        "      mnist_model.eval()\n",
        "      avg_loss = np.mean(losses)\n",
        "      train_losses.append(avg_loss)\n",
        "      time_elapsed = time.time() - start_time\n",
        "      print(f\"Epoch [{epoch + 1}/{args.epochs}], Loss: {avg_loss:.4f}, Time(s): {time_elapsed:.2f}\")\n",
        "\n",
        "      # Performance with validation set\n",
        "      correct = 0.0\n",
        "      total = 0.0\n",
        "      losses = []\n",
        "      for i, (input, label) in enumerate(self._test_dataloader):\n",
        "        predictions = mnist_model(input)\n",
        "        loss = loss_func(predictions, label)\n",
        "        losses.append(loss.data.item())\n",
        "        _, predicted = torch.max(input=predictions.data, dim=1)\n",
        "        \n",
        "        correct += (predicted == label).sum()\n",
        "        total += label.size(0)\n",
        "\n",
        "      \n",
        "      val_loss = np.mean(losses)      \n",
        "      val_acc = 100.0 * correct / total\n",
        "      val_losses.append(val_loss)\n",
        "      val_accuracies.append(val_acc)\n",
        "      time_elapsed = time.time() - start_time\n",
        "      print(f\"Epoch [{epoch + 1}/{args.epochs}], Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}, Time(s): {time_elapsed:.2f}\")\n",
        "\n",
        "\n",
        "    return mnist_model, [train_losses, val_losses, val_accuracies]\n"
      ],
      "metadata": {
        "id": "hnI1HyYj1oi6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Convolutional Neural Network "
      ],
      "metadata": {
        "id": "7OcHJbNGdqIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Model\n",
        "\n",
        "This model consists of 2 convolution blocks, and a linear classifier. The convolution blocks are the following layers in sequence:\n",
        "* Convolution layer\n",
        "* Max pooling layer\n",
        "* Batch Norm layer\n",
        "* ReLU activation layer"
      ],
      "metadata": {
        "id": "I-E0b-Qui2k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_CNN(nn.Module):\n",
        "  \"\"\"\n",
        "  A convolutional neural network\n",
        "  \"\"\"\n",
        "  def __init__(self, image_size, num_classes, num_in_channels, train_args) -> None:\n",
        "      super().__init__()\n",
        "\n",
        "      kernel = train_args.kernel\n",
        "      num_filters = train_args.num_filters\n",
        "\n",
        "      padding = kernel // 2\n",
        "\n",
        "      self.convBlock1 = nn.Sequential(\n",
        "          nn.Conv2d(\n",
        "              in_channels=num_in_channels, \n",
        "              out_channels=num_filters,\n",
        "              kernel_size=kernel,\n",
        "              padding=padding),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(num_features=num_filters),\n",
        "          nn.ReLU())\n",
        "      \n",
        "      self.convBlock2 = nn.Sequential(\n",
        "          nn.Conv2d(\n",
        "              in_channels=num_filters, \n",
        "              out_channels=num_filters*2,\n",
        "              kernel_size=kernel,\n",
        "              padding=padding),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(num_features=num_filters*2),\n",
        "          nn.ReLU())\n",
        "      \n",
        "      self.flatten = nn.Flatten()\n",
        "\n",
        "      in_features = (num_filters*2)*(image_size//4)*(image_size//4)\n",
        "\n",
        "      self.linearBlock = nn.Sequential(\n",
        "          nn.Linear(in_features=in_features, out_features=num_classes),\n",
        "      )\n",
        "      \n",
        "\n",
        "  def forward(self, x):\n",
        "    convolved1 = self.convBlock1(x)\n",
        "    convolved2 = self.convBlock2(convolved1)\n",
        "    flattened = self.flatten(convolved2)\n",
        "    prediction = self.linearBlock(flattened)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "kXVPQqF3d1UF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "_v8diKXZjrP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIG\n",
        "cnn_args = Config()\n",
        "cnn_args_dict = {\n",
        "    \"experiment_name\": \"MNIST_digit_classification_CNN\",\n",
        "    \"Model\": MNIST_CNN,\n",
        "    \"seed\": 0,\n",
        "    \"epochs\": 25,\n",
        "    \"learn_rate\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"kernel\": 5,\n",
        "    \"num_filters\": 16,\n",
        "}\n",
        "cnn_args.update(cnn_args_dict)\n",
        "\n",
        "trainer = MNIST_Classify()\n",
        "cnn_model, _ = trainer.train_model(cnn_args)"
      ],
      "metadata": {
        "id": "8Lb6mlnej2Lx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f45434-1752-4366-ac8e-3f920e4d7b32"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================== MNIST_digit_classification_CNN ===================\n",
            "Preparing Data...\n",
            "Beginning Training...\n",
            "Epoch [1/25], Loss: 0.1036, Time(s): 34.64\n",
            "Epoch [1/25], Val Loss: 0.0396, Val Acc: 98.78, Time(s): 37.46\n",
            "Epoch [2/25], Loss: 0.0358, Time(s): 71.41\n",
            "Epoch [2/25], Val Loss: 0.0313, Val Acc: 98.95, Time(s): 74.24\n",
            "Epoch [3/25], Loss: 0.0253, Time(s): 108.34\n",
            "Epoch [3/25], Val Loss: 0.0416, Val Acc: 98.71, Time(s): 111.21\n",
            "Epoch [4/25], Loss: 0.0184, Time(s): 145.26\n",
            "Epoch [4/25], Val Loss: 0.0271, Val Acc: 99.09, Time(s): 148.10\n",
            "Epoch [5/25], Loss: 0.0139, Time(s): 182.20\n",
            "Epoch [5/25], Val Loss: 0.0302, Val Acc: 98.93, Time(s): 185.01\n",
            "Epoch [6/25], Loss: 0.0137, Time(s): 218.31\n",
            "Epoch [6/25], Val Loss: 0.0280, Val Acc: 99.23, Time(s): 221.12\n",
            "Epoch [7/25], Loss: 0.0097, Time(s): 254.38\n",
            "Epoch [7/25], Val Loss: 0.0271, Val Acc: 99.20, Time(s): 257.15\n",
            "Epoch [8/25], Loss: 0.0068, Time(s): 290.42\n",
            "Epoch [8/25], Val Loss: 0.0307, Val Acc: 99.13, Time(s): 293.21\n",
            "Epoch [9/25], Loss: 0.0085, Time(s): 326.51\n",
            "Epoch [9/25], Val Loss: 0.0402, Val Acc: 98.91, Time(s): 329.28\n",
            "Epoch [10/25], Loss: 0.0064, Time(s): 362.44\n",
            "Epoch [10/25], Val Loss: 0.0387, Val Acc: 98.98, Time(s): 365.22\n",
            "Epoch [11/25], Loss: 0.0053, Time(s): 398.44\n",
            "Epoch [11/25], Val Loss: 0.0364, Val Acc: 98.98, Time(s): 401.23\n",
            "Epoch [12/25], Loss: 0.0059, Time(s): 434.43\n",
            "Epoch [12/25], Val Loss: 0.0330, Val Acc: 99.17, Time(s): 437.23\n",
            "Epoch [13/25], Loss: 0.0039, Time(s): 470.77\n",
            "Epoch [13/25], Val Loss: 0.0326, Val Acc: 99.14, Time(s): 473.63\n",
            "Epoch [14/25], Loss: 0.0044, Time(s): 507.67\n",
            "Epoch [14/25], Val Loss: 0.0384, Val Acc: 99.13, Time(s): 510.52\n",
            "Epoch [15/25], Loss: 0.0034, Time(s): 544.76\n",
            "Epoch [15/25], Val Loss: 0.0522, Val Acc: 98.72, Time(s): 547.61\n",
            "Epoch [16/25], Loss: 0.0032, Time(s): 581.55\n",
            "Epoch [16/25], Val Loss: 0.0389, Val Acc: 99.07, Time(s): 584.33\n",
            "Epoch [17/25], Loss: 0.0034, Time(s): 617.90\n",
            "Epoch [17/25], Val Loss: 0.0536, Val Acc: 98.81, Time(s): 620.66\n",
            "Epoch [18/25], Loss: 0.0039, Time(s): 654.27\n",
            "Epoch [18/25], Val Loss: 0.0403, Val Acc: 99.21, Time(s): 657.09\n",
            "Epoch [19/25], Loss: 0.0028, Time(s): 690.64\n",
            "Epoch [19/25], Val Loss: 0.0422, Val Acc: 99.14, Time(s): 693.49\n",
            "Epoch [20/25], Loss: 0.0026, Time(s): 727.15\n",
            "Epoch [20/25], Val Loss: 0.0514, Val Acc: 99.00, Time(s): 729.94\n",
            "Epoch [21/25], Loss: 0.0026, Time(s): 763.75\n",
            "Epoch [21/25], Val Loss: 0.0376, Val Acc: 99.25, Time(s): 766.57\n",
            "Epoch [22/25], Loss: 0.0029, Time(s): 800.59\n",
            "Epoch [22/25], Val Loss: 0.0440, Val Acc: 99.14, Time(s): 803.47\n",
            "Epoch [23/25], Loss: 0.0013, Time(s): 837.51\n",
            "Epoch [23/25], Val Loss: 0.0454, Val Acc: 98.99, Time(s): 840.31\n",
            "Epoch [24/25], Loss: 0.0016, Time(s): 874.01\n",
            "Epoch [24/25], Val Loss: 0.0414, Val Acc: 99.17, Time(s): 876.76\n",
            "Epoch [25/25], Loss: 0.0032, Time(s): 909.94\n",
            "Epoch [25/25], Val Loss: 0.0402, Val Acc: 99.13, Time(s): 912.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. AutoEncoders"
      ],
      "metadata": {
        "id": "CEU5nudYkhaW"
      }
    }
  ]
}