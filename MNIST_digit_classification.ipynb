{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_digit_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMClzHxOP7aBuchu8QY3L3S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertkaya1033/MNIST_digit_classify/blob/master/MNIST_digit_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Handwritten Digit Classification\n"
      ],
      "metadata": {
        "id": "ozGR_gDWdZSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Helpers"
      ],
      "metadata": {
        "id": "0zr80rP9kJde"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZCD4-n8wz9D",
        "outputId": "545ca207-b2f7-4c41-e005-3840c2655a85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "# https://stackoverflow.com/questions/39832721/meaning-of-self-dict-self-in-a-class-definition\n",
        "class Config(dict):\n",
        "  \"\"\"A class that allows attribute access and item access.\"\"\"\n",
        "  def __init__(self):\n",
        "    self.__dict__ = self\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network "
      ],
      "metadata": {
        "id": "7OcHJbNGdqIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_CNN(nn.Module):\n",
        "  \"\"\"\n",
        "  A convolutional neural network\n",
        "  \"\"\"\n",
        "  def __init__(self, image_size, num_classes, num_in_channels, train_args) -> None:\n",
        "      super().__init__()\n",
        "\n",
        "      kernel = train_args.kernel\n",
        "      num_filters = train_args.num_filters\n",
        "\n",
        "      padding = kernel // 2\n",
        "\n",
        "      self.convBlock1 = nn.Sequential(\n",
        "          nn.Conv2d(\n",
        "              in_channels=num_in_channels, \n",
        "              out_channels=num_filters,\n",
        "              kernel_size=kernel,\n",
        "              padding=padding),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(num_features=num_filters),\n",
        "          nn.ReLU())\n",
        "      \n",
        "      self.convBlock2 = nn.Sequential(\n",
        "          nn.Conv2d(\n",
        "              in_channels=num_filters, \n",
        "              out_channels=num_filters*2,\n",
        "              kernel_size=kernel,\n",
        "              padding=padding),\n",
        "          nn.MaxPool2d(kernel_size=2),\n",
        "          nn.BatchNorm2d(num_features=num_filters*2),\n",
        "          nn.ReLU())\n",
        "      \n",
        "      self.flatten = nn.Flatten()\n",
        "\n",
        "      in_features = (num_filters*2)*(image_size//4)*(image_size//4)\n",
        "\n",
        "      self.linearBlock = nn.Sequential(\n",
        "          nn.Linear(in_features=in_features, out_features=num_classes),\n",
        "      )\n",
        "      \n",
        "\n",
        "  def forward(self, x):\n",
        "    convolved1 = self.convBlock1(x)\n",
        "    convolved2 = self.convBlock2(convolved1)\n",
        "    flattened = self.flatten(convolved2)\n",
        "    prediction = self.linearBlock(flattened)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "kXVPQqF3d1UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Digit Classification Trainer"
      ],
      "metadata": {
        "id": "9D283ewe17lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_Classify():\n",
        "  \"\"\"Downloads the MNIST dataset, and trains models that predict the handwritten\n",
        "  digit written on the provided 28x28 image.\"\"\"\n",
        "\n",
        "  _training_data = None\n",
        "  _test_data = None\n",
        "\n",
        "  @staticmethod\n",
        "  def load_data(root: str = \"data\") -> Tuple[Dataset, Dataset]:\n",
        "    \"\"\"Downloads MNIST dataset into local file system, into the root directory. \n",
        "    Loads the dataset into memory.\n",
        "    \n",
        "    Parameters:\n",
        "      root (str) -- the directory to download the dataset. (default: \"data\")\n",
        "    \n",
        "    Returns:\n",
        "      tuple --  (training dataset, test dataset)\n",
        "    \"\"\"\n",
        "    if MNIST_Classify._training_data is None:\n",
        "      MNIST_Classify._training_data = datasets.MNIST(\n",
        "          root=root,\n",
        "          train=True,\n",
        "          download=True,\n",
        "          transform=ToTensor()\n",
        "      )\n",
        "    if MNIST_Classify._test_data is None:\n",
        "      MNIST_Classify._test_data = datasets.MNIST(\n",
        "          root=root,\n",
        "          train=False,\n",
        "          download=True,\n",
        "          transform=ToTensor()\n",
        "      )\n",
        "    return MNIST_Classify._training_data, MNIST_Classify._test_data\n",
        "    \n",
        "  \n",
        "  def _prepare_data(self, args: Config) -> None:\n",
        "    \"\"\"Prepares the dataset for training.\n",
        "\n",
        "    Parameters:\n",
        "        args (Config) --  hyperparameters of the model being trained\n",
        "    \"\"\"\n",
        "    MNIST_Classify.load_data()\n",
        "\n",
        "    self._train_dataloader = DataLoader(MNIST_Classify._training_data, batch_size=args.batch_size, shuffle=True)\n",
        "    self._test_dataloader = DataLoader(MNIST_Classify._test_data, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "  def train_model(self, args: Config):\n",
        "    \"\"\"Trains an instance of a model provided in args with the given hyperparameters\n",
        "    for the task of classifying handwritten digits using the MNIST dataset.\n",
        "\n",
        "    Parameters:\n",
        "        args (Config) --  stores the model and the hyperparameters in which the \n",
        "                          model instance should be trained\n",
        "\n",
        "    Returns:\n",
        "        The trained model instance\n",
        "    \"\"\"\n",
        "    self._prepare_data(args)\n",
        "    \n",
        "    num_digits = 10\n",
        "    num_channels = 1\n",
        "    image_size = MNIST_Classify._training_data.data.size()[1] #28x28\n",
        "    num_batches = len(self._train_dataloader)\n",
        "\n",
        "    mnist_model = args.Model(image_size, num_digits, num_channels, args)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(params=mnist_model.parameters(), lr=args.learn_rate)\n",
        "    \n",
        "    mnist_model.train()\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "      for i, (input, label) in enumerate(self._train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = mnist_model(input)\n",
        "        loss = loss_func(predictions, label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{args.epochs}], Step [{i+1}/{num_batches}], Loss: {loss.item():.4f}')\n",
        "    return mnist_model\n"
      ],
      "metadata": {
        "id": "hnI1HyYj1oi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## h"
      ],
      "metadata": {
        "id": "_v8diKXZjrP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIG\n",
        "cnn_args = Config()\n",
        "cnn_args_dict = {\n",
        "    \"experiment_name\": \"MNIST_digit_classification_CNN\",\n",
        "    \"Model\": MNIST_CNN,\n",
        "    \"seed\": 0,\n",
        "    \"epochs\": 10,\n",
        "    \"learn_rate\": 0.001,\n",
        "    \"batch_size\": 64,\n",
        "    \"kernel\": 5,\n",
        "    \"num_filters\": 16,\n",
        "}\n",
        "cnn_args.update(cnn_args_dict)\n",
        "\n",
        "trainer = MNIST_Classify()\n",
        "cnn_model = trainer.train_model(cnn_args)"
      ],
      "metadata": {
        "id": "8Lb6mlnej2Lx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}